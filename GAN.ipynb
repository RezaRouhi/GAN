{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjqqBbbqqbhM"
   },
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.batch_size = None\n",
    "        self.timestep = xtrain.shape[1]\n",
    "        self.feature = xtrain.shape[2]\n",
    "        optimizer = Adam(0.01,0.99)\n",
    "        self.landa1 = 1\n",
    "        self.landa2 = 1\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(optimizer = optimizer , loss = 'binary_crossentropy')\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        # The generator takes real data as input and generates data\n",
    "        gen_output = self.generator(self.inputs)\n",
    "        # The discriminator takes generated data as input and determines dis_output\n",
    "        concat = concatenate([self.inputs,gen_output], axis = 1)\n",
    "        dis_input = Reshape(((self.timestep+1)*self.feature,))(concat)\n",
    "        dis_output = self.discriminator(dis_input)\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        self.combined = Model(self.inputs, dis_output)\n",
    "        self.combined.compile(optimizer = optimizer, loss = 'binary_crossentropy')\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        self.inputs = Input(batch_shape = (self.batch_size, self.timestep, self.feature))\n",
    "        lstm1 = LSTM(256, return_sequences = True, stateful = False)(self.inputs)\n",
    "        batch1 = BatchNormalization()(lstm1)\n",
    "        drop1 = Dropout(0.2)(batch1)\n",
    "        lstm2 = LSTM(128, return_sequences = True, stateful = False)(drop1)\n",
    "        batch2 = BatchNormalization()(lstm2)\n",
    "        drop2 = Dropout(0.2)(batch2)\n",
    "        lstm3 = LSTM(64, return_sequences = True, stateful = False)(drop2)\n",
    "        batch3 = BatchNormalization()(lstm3)\n",
    "        drop3 = Dropout(0.2)(batch3)\n",
    "        lstm4 = LSTM(1, return_sequences = True, stateful = False)(drop3)\n",
    "        batch4 = BatchNormalization()(lstm4)\n",
    "        drop4 = Dropout(0.2)(batch4)\n",
    "        fl = Flatten()(drop4)\n",
    "        re = Reshape((1,22))(fl)\n",
    "        rp = Dense(19)(re)\n",
    "        return Model(self.inputs,rp)\n",
    "   \n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 128, activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(units = 64, activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(units = 32, activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(units = 1, activation = 'relu'))\n",
    "        d_input = Input(batch_shape = (self.batch_size, ((self.timestep+1)*self.feature)))\n",
    "        d_output = model(d_input)\n",
    "        return Model(d_input, d_output)\n",
    "    \n",
    "    def train(self, epochs, batch_size):\n",
    "        self.D_loss = []\n",
    "        self.g_loss = []\n",
    "        self.g_MSE = []\n",
    "        self.G_loss= []\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0,xtrain.shape[0]//batch_size):\n",
    "                x_train_batch = xtrain[batch_size*i:batch_size*i+batch_size,:,:]\n",
    "                y_train_batch = ytrain[batch_size*i:batch_size*i+batch_size,:,:]\n",
    "                y_pred_batch = self.generator.predict(x_train_batch)\n",
    "                \n",
    "                \n",
    "                real = np.concatenate((x_train_batch,y_train_batch), axis=1)\n",
    "                fake = np.concatenate((x_train_batch,y_pred_batch), axis=1)\n",
    "                label_real = np.ones((real.shape[0],1))\n",
    "                label_fake = np.zeros((fake.shape[0],1))\n",
    "                real = np.reshape(real, (batch_size, (self.timestep+1)*self.feature))\n",
    "                fake = np.reshape(fake, (batch_size, (self.timestep+1)*self.feature))\n",
    "                \n",
    "                # Train the discriminator\n",
    "#                 self.discriminator.trainable = True\n",
    "                \n",
    "                # Train_on_batch saves the previous weights for each i iteration\n",
    "                d_loss_real = self.discriminator.train_on_batch(real, label_real) \n",
    "                d_loss_fake = self.discriminator.train_on_batch(fake, label_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                self.D_loss.append(d_loss)\n",
    "                D_loss = np.array(self.D_loss)\n",
    "                \n",
    "                \n",
    "                # Train the generator (to have the discriminator label samples as label_real)\n",
    "                # For the combined model we will only train the generator\n",
    "                \n",
    "                gan_loss = self.combined.train_on_batch(x_train_batch, label_fake)\n",
    "                x_pred_g = self.generator.predict(x_train_batch)\n",
    "                m = xtrain.shape[0]//batch_size\n",
    "                self.g_MSE.append(np.sum((1/m)*(x_pred_g[:,0,4:5]-y_train_batch[:,0,4:5])**2))\n",
    "                self.g_loss.append(gan_loss)\n",
    "                self.g_MSE1 = np.array(self.g_MSE)\n",
    "                self.g_loss1 = np.array(self.g_loss)\n",
    "                self.G_loss = np.add(self.landa1*self.g_loss1,self.landa2*self.g_MSE1)\n",
    "                # Print the Results\n",
    "                print (\"epoch : %d , batch : %d , GAN loss : %f\" % (epoch, i, gan_loss))\n",
    "            self.combined.reset_states()\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        self.predicted = self.generator.predict(xtest)\n",
    "        self.predicted = np.array(self.predicted)\n",
    "        \n",
    "        \n",
    "    def plot(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.D_loss, color = 'blue')\n",
    "        plt.ylabel(\"Discriminator Loss\", fontsize=13)\n",
    "        plt.xlabel('Iterations of Batch', fontsize=13)\n",
    "        fig1 = plt.gcf()\n",
    "        fig1.savefig('diss_loss.png', dpi=500, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(self.G_loss, color = 'red')\n",
    "        plt.ylabel(\"Generator Loss\", fontsize=13)\n",
    "        plt.xlabel('Iterations of Batch', fontsize=13)\n",
    "        fig2 = plt.gcf()\n",
    "        fig2.savefig('gen_loss.png', dpi=500, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs = 5, batch_size = 32)\n",
    "    gan.predict()\n",
    "    gan.plot()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KTpCmCjLqbfu",
    "NcslKmBkqbf3",
    "vyP656Q1qbf-"
   ],
   "name": "GAN-5Step-Wav.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
